{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from dataset import *\n",
    "from data_handler import *\n",
    "from embeddings import *\n",
    "from vector_store import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets: ['ConvFinQA', 'FinQA', 'MultiHeritt', 'TATQA']\n"
     ]
    }
   ],
   "source": [
    "dataset_manager = FinanceRAGDataset(\"../data\")\n",
    "\n",
    "# List available datasets\n",
    "print(\"Available datasets:\", dataset_manager.list_datasets())\n",
    "\n",
    "# Load corpus and queries from a specific dataset\n",
    "DATASET_NAME = \"ConvFinQA\"\n",
    "convfinqa_corpus, convfinqa_queries, convfinqa_qrels = dataset_manager.load_dataset(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tables from the corpus\n",
    "tables = {}\n",
    "for doc in convfinqa_corpus:\n",
    "    tables[doc['_id']] = extract_tables(doc['text'])\n",
    "\n",
    "# # Save\n",
    "# np.save('extracted_tables.npy', tables) \n",
    "# # Load\n",
    "# read_dictionary = np.load('extracted_tables.npy',allow_pickle='TRUE').item()\n",
    "# print(read_dictionary['dd4bff516'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ProsusAI/finbert were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (522 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "text_processor = DataHandler(Tokenizer(AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")),\n",
    "                             Embedder(AutoModel.from_pretrained(\"ProsusAI/finbert\")))\n",
    "\n",
    "embedding = {}\n",
    "for idx, documnet in enumerate(convfinqa_corpus):\n",
    "    print(idx)\n",
    "    embedded_doc = (\n",
    "        text_processor.load_data(documnet['text'])\n",
    "        .to_lowercase()\n",
    "        .remove_punctuation()\n",
    "        .remove_stopwords()\n",
    "        .lemmatize_text()\n",
    "        .remove_extra_whitespace()\n",
    "        .tokenize()\n",
    "        .embed()\n",
    "        .get_data()\n",
    "    )\n",
    "    embedding[documnet['_id']] = embedded_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 768\n",
    "vector_store = FaissVectorStore(dimension=EMBEDDING_DIM)\n",
    "# Populate vector store\n",
    "for i, doc in embedding.items():\n",
    "    # Add vectors to store\n",
    "    vector_store.add_embeddings(np.array(doc), [i for e in range(len(doc))], [{'chunk': e} for e in range(len(doc))])\n",
    "# Save vector store to file\n",
    "vector_store.save(f'../vector_store/{DATASET_NAME}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance: 445.6521911621094\n",
      "Text: dd4bff516\n",
      "Metadata: {'chunk': 1}\n",
      "---\n",
      "Distance: 451.95758056640625\n",
      "Text: dd4b93b5e\n",
      "Metadata: {'chunk': 1}\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Simulate a query embedding\n",
    "query_embedding = np.random.rand(1, EMBEDDING_DIM)\n",
    "\n",
    "# Perform similarity search\n",
    "distances, indices, result_texts, result_metadata = vector_store.similarity_search(\n",
    "    query_embedding, \n",
    "    k=2,\n",
    "    filter_metadata={\"category\": \"AI\"}\n",
    ")\n",
    "\n",
    "# Print results\n",
    "for dist, text, meta in zip(distances[0], result_texts, result_metadata):\n",
    "    print(f\"Distance: {dist}\")\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Metadata: {meta}\")\n",
    "    print(\"---\")\n",
    "\n",
    "# Save vector store\n",
    "# vector_store.save(f'../vector_store/{DATASET_NAME}')\n",
    "\n",
    "# Load vector store\n",
    "# loaded_store = FaissVectorStore.load(f'../vector_store/{DATASET_NAME}', dimension=EMBEDDING_DIM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
