{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from dataset import *\n",
    "from data_handler import *\n",
    "from embeddings import *\n",
    "from vector_store import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets: ['ConvFinQA', 'FinQA', 'MultiHeritt', 'TATQA']\n"
     ]
    }
   ],
   "source": [
    "dataset_manager = FinanceRAGDataset(\"../data\")\n",
    "\n",
    "# List available datasets\n",
    "print(\"Available datasets:\", dataset_manager.list_datasets())\n",
    "\n",
    "# Load corpus and queries from a specific dataset\n",
    "DATASET_NAME = \"TATQA\"\n",
    "convfinqa_corpus, convfinqa_queries, convfinqa_qrels = dataset_manager.load_dataset(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only docs with labels\n",
    "doc_ids = np.array([query[\"_id\"] for query in convfinqa_corpus])\n",
    "doc_texts = np.array([query[\"text\"] for query in convfinqa_corpus])\n",
    "mask_queries = np.isin(doc_ids, np.unique(convfinqa_qrels[\"corpus_id\"]))\n",
    "doc_ids = doc_ids[mask_queries]\n",
    "doc_texts = [[doc] for doc in doc_texts[mask_queries]]\n",
    "corpus = dict(zip(doc_ids, doc_texts))\n",
    "\n",
    "# Get the queries\n",
    "query_ids = np.array([query[\"_id\"] for query in convfinqa_queries])\n",
    "query_texts = np.array([query[\"text\"] for query in convfinqa_queries])\n",
    "mask_queries = np.isin(query_ids, np.unique(convfinqa_qrels[\"query_id\"]))\n",
    "query_ids = query_ids[mask_queries]\n",
    "query_texts = query_texts[mask_queries]\n",
    "queries = dict(zip(query_ids, query_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248 498\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus), len(queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract tables from the corpus\n",
    "# tables = {}\n",
    "# for id, doc in corpus.items():\n",
    "#     tables[id] = extract_tables(doc[0])\n",
    "\n",
    "# # Save\n",
    "# np.save(f'extracted_tables_{DATASET_NAME}.npy', tables) \n",
    "# # Load\n",
    "# # read_dictionary = np.load('extracted_tables_{DATASET_NAME}.npy',allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load extracted tables\n",
    "table_summaries = np.load('table_summaries.npy', allow_pickle='TRUE').item()\n",
    "# Add table summaries to corpus\n",
    "for idx, text in table_summaries.items():\n",
    "    corpus[idx].append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (760 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_processor = DataHandler(Tokenizer(AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")),\n",
    "                             Embedder(AutoModel.from_pretrained(\"ProsusAI/finbert\")))\n",
    "\n",
    "corpus_embedding = {}\n",
    "for idx, text in corpus.items():\n",
    "    embedded_doc = (\n",
    "        text_processor.load_data(text)\n",
    "        # .to_lowercase()\n",
    "        # .remove_punctuation()\n",
    "        # .remove_numbers()\n",
    "        # .remove_stopwords()\n",
    "        # .lemmatize_text()\n",
    "        # .remove_extra_whitespace()\n",
    "        .tokenize()\n",
    "        .chunk_split(max_length=512)\n",
    "        .embed(method='last_layer')\n",
    "        .get_data()\n",
    "    )\n",
    "    corpus_embedding[idx] = embedded_doc\n",
    "\n",
    "EMBEDDING_DIM = 768\n",
    "vector_store = FaissVectorStore(dimension=EMBEDDING_DIM)\n",
    "# Populate vector store\n",
    "for i, doc in corpus_embedding.items():\n",
    "    # Add vectors to store\n",
    "    vector_store.add_embeddings(np.array(doc), [i for e in range(len(doc))], [{'chunk': e} for e in range(len(doc))])\n",
    "# Save vector store to file\n",
    "vector_store.save(f'../vector_store/{DATASET_NAME}')\n",
    "\n",
    "queries_embedding = {}\n",
    "for idx, text in queries.items():\n",
    "    embedded_query = (\n",
    "        text_processor.load_data(text)\n",
    "        # .to_lowercase()\n",
    "        # .remove_punctuation()\n",
    "        # .remove_stopwords()\n",
    "        # .lemmatize_text()\n",
    "        # .remove_extra_whitespace()\n",
    "        .tokenize()\n",
    "        .chunk_split(max_length=512)\n",
    "        .embed(method='last_layer')\n",
    "        .get_data()\n",
    "    )\n",
    "    queries_embedding[idx] = embedded_query\n",
    "\n",
    "best_match = {}\n",
    "for query_id, query in queries_embedding.items():\n",
    "    distances, indices, result_texts, result_metadata = vector_store.similarity_search(\n",
    "        query, \n",
    "        k=10\n",
    "    )\n",
    "    best_match[query_id] = result_texts\n",
    "\n",
    "n_match = 0\n",
    "for query_id, result in best_match.items():\n",
    "    idx = np.argmax(convfinqa_qrels['query_id']==query_id)\n",
    "    if convfinqa_qrels['corpus_id'][idx] in result:\n",
    "        n_match += 1\n",
    "\n",
    "n_match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (878 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "331"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_processor = DataHandler(Tokenizer(AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")),\n",
    "                             Embedder(AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")))\n",
    "corpus_embedding = {}\n",
    "for idx, text in corpus.items():\n",
    "    embedded_doc = (\n",
    "        text_processor.load_data(text)\n",
    "        # .to_lowercase()\n",
    "        # .remove_numbers()\n",
    "        # .remove_punctuation()\n",
    "        # .remove_stopwords()\n",
    "        # .lemmatize_text()\n",
    "        # .remove_extra_whitespace()\n",
    "        .tokenize()\n",
    "        .chunk_split(max_length=256)\n",
    "        .embed()\n",
    "        .get_data()\n",
    "    )\n",
    "    # embedded_doc = model.encode(text)\n",
    "    corpus_embedding[idx] = embedded_doc\n",
    "\n",
    "EMBEDDING_DIM = 384\n",
    "vector_store = FaissVectorStore(dimension=EMBEDDING_DIM)\n",
    "# Populate vector store\n",
    "for i, doc in corpus_embedding.items():\n",
    "    # Add vectors to store\n",
    "    vector_store.add_embeddings(np.array(doc), [i for e in range(len(doc))], [{'chunk': e} for e in range(len(doc))])\n",
    "# Save vector store to file\n",
    "vector_store.save(f'../vector_store/{DATASET_NAME}')\n",
    "\n",
    "queries_embedding = {}\n",
    "for idx, text in queries.items():\n",
    "    embedded_query = (\n",
    "        text_processor.load_data(text)\n",
    "        # .to_lowercase()\n",
    "        # .remove_numbers()\n",
    "        # .remove_punctuation()\n",
    "        # .remove_stopwords()\n",
    "        # .lemmatize_text()\n",
    "        # .remove_extra_whitespace()\n",
    "        .tokenize()\n",
    "        .chunk_split(max_length=256)\n",
    "        .embed()\n",
    "        .get_data()\n",
    "    )\n",
    "    queries_embedding[idx] = embedded_query\n",
    "\n",
    "best_match = {}\n",
    "for query_id, query in queries_embedding.items():\n",
    "    distances, indices, result_texts, result_metadata = vector_store.similarity_search(\n",
    "        query, \n",
    "        k=10\n",
    "    )\n",
    "    best_match[query_id] = result_texts\n",
    "\n",
    "n_match = 0\n",
    "for query_id, result in best_match.items():\n",
    "    idx = np.argmax(convfinqa_qrels['query_id']==query_id)\n",
    "    if convfinqa_qrels['corpus_id'][idx] in result:\n",
    "        n_match += 1\n",
    "\n",
    "n_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6646586345381527"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_match/len(queries)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
